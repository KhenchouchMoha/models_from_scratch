{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "welcome to this collab notebook .\n",
        "in this notebook, i tried to code from scratch multilayer perceptron , layer for GRU and LSTM .\n",
        "theses classes, are implemented  only with numpt and based on the mathematical background of the models, there is somes small problems in multi_layer_perceptron , but sooner i will get the solution and turn the class be more operational in practical uses in real applications.\n",
        "so, my goal here, was : how neural network and deep learning models works ? what's is the mathematical background of these models ?"
      ],
      "metadata": {
        "id": "leLy9YP1D5bV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**note : **  \n",
        "*   i did'nt code how to train these models,(optimizations algorithms), i will do that in the futur notebook . inshaeallah .\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "3M76X0xCp1A_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   i will so happy, if this code give some new informations about these neural network and layers , so i'm also open for any questions, remark or new proposition . my goal is to learn more and more in the AI field .\n",
        "\n",
        "*made by : Khenchouch Mohamed*\n",
        "\n"
      ],
      "metadata": {
        "id": "hhJQna9kqMaZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*this section for feed forward neural network *"
      ],
      "metadata": {
        "id": "ZpBWYSlmru2h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Multilayer perceptron"
      ],
      "metadata": {
        "id": "OiWtlPT6EeM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "ZRWR_E2lGqiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuydrUG3_ZBI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from numpy.random import default_rng\n",
        "\n",
        "class Multi_layer_perceptron() :\n",
        "\n",
        "  def __init__ (self,number_of_inputs,number_of_outputs,number_of_hiddens,learning_rate) :\n",
        "    self.W1=default_rng(42).random((number_of_inputs,number_of_hiddens))\n",
        "    self.b1=np.zeros(number_of_hiddens)\n",
        "    self.W2=default_rng(42).random((number_of_hiddens,number_of_outputs))\n",
        "    self.b2=np.zeros(number_of_outputs)\n",
        "#activation function\n",
        "  def Relu(x) :\n",
        "    value=np.zeros(x.shape)\n",
        "    return np.max(value,x)\n",
        "\n",
        "  def add_layer(self,number_of_hiddens1,num) :\n",
        "    self.W2.reshape(self.W2.shape[0],number_of_hiddens1)\n",
        "    self.b2.reshape(number_of_hiddens1)\n",
        "    self.aide=default_rng(42).random(number_of_hiddens1,self.W2.shape[1])\n",
        "    self.b=np.zeros(self.number_of_outputs)\n",
        "    return (self.aide,self.b)\n",
        "\n",
        "  def forward(self,X) :\n",
        "    X=X.reshape(-1,self.number_of_inputs)\n",
        "    H=self.Relu(np.matmult(X,self.W1) +self.b1)\n",
        "    return np.matmult(H,self.W2)+self.b1\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**This section for Reccurent neural Network **"
      ],
      "metadata": {
        "id": "TgAjW9DSrnJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU LAYER"
      ],
      "metadata": {
        "id": "y0yiJl_s63PB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_sigmoid(x) :\n",
        "  return 1/(1+np.exp(-x))"
      ],
      "metadata": {
        "id": "peSxPGJH-Geg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GRU():\n",
        "  def __init__(self,number_of_inputs,number_of_hiddens,sigma) :\n",
        "    self.Wxr=default_rng(42).random((number_of_inputs,number_of_hiddens))\n",
        "    self.Wxz=default_rng(42).random((number_of_inputs,number_of_hiddens))\n",
        "    self.Whr=default_rng(42).random((number_of_hiddens,number_of_hiddens))\n",
        "    self.Whz=default_rng(42).random((number_of_inputs,number_of_hiddens))\n",
        "    self.Whh=default_rng(42).random((number_of_inputs,number_of_hiddens))\n",
        "    self.br=np.zeros(number_of_hiddens)\n",
        "    self.bz=np.zeros(number_of_hiddens)\n",
        "    self.bh=np.zeros(number_of_hiddens)\n",
        "  # Reset gate : a reset would allow us to control how much of the previous state we might still want to remember.\n",
        "  def Reset_Gate(self,X,Ht_1) :\n",
        "    return logistic_sigmoid(np.matmul(X,self.Wxr)+np.matmul(Ht_1,self.Whr)+self.br)\n",
        "  # Update gate : update gate would allow us to control how much of the new state is just a copy of the old state.\n",
        "  def Update_Gate(self,X,Ht_1) :\n",
        "    return logistic_sigmoid(np.matmul(X,self.Wxz)+np.matmul(Ht_1,self.Whz)+self.bz)\n",
        "  # candidate hidden state : the integration between the Reset gate and the regular latent state updating mechnism leads to the candidate hidden state\n",
        "  def candidate_Hidden_State(self,X,Ht_1) :\n",
        "    return np.tanh(np.matmul(X,self.wxh)+np.matmul(np.multiply(self.Reset_Gate(X,Ht_1)),self.Whh)+self.bh)\n",
        "  # Hidden state :\n",
        "  def Hidden_State(self,Ht_1) :\n",
        "    return np.multiply(self.Update_Gate,Ht_1)+np.multiply((1-self.Update_Gate),self.candidate_Hidden_State)\n",
        "  # Forward method :\n",
        "  def forward(self,X,Ht_1) :\n",
        "    Rt=self.Reset_Gate(X,Ht_1)\n",
        "    Zt=self.Update_Gate(X,Ht_1)\n",
        "    Ht=self.Hidden_State(Ht_1)\n",
        "    return Rt,Zt,Ht"
      ],
      "metadata": {
        "id": "48IAo80Y6678"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Long short term memory"
      ],
      "metadata": {
        "id": "hBj2nYK7_CNp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Long_short_term_memory() :\n",
        "  def __init__(self,number_of_inputs,number_of_hiddens,sigma) :\n",
        "    self.Wxi =default_rng(42).random((number_of_inputs,number_of_hiddens))\n",
        "    self.Whi =default_rng(42).random((number_of_hiddens,number_of_hiddens))\n",
        "    self.Wxf =default_rng(42).random((number_of_inputs,number_of_hiddens))\n",
        "    self.Whf =default_rng(42).random((number_of_hiddens,number_of_hiddens))\n",
        "    self.Wxo =default_rng(42).random((number_of_inputs,number_of_hiddens))\n",
        "    self.Who =default_rng(42).random((number_of_hiddens,number_of_hiddens))\n",
        "    self.Wxc =default_rng(42).random((number_of_inputs,number_of_hiddens))\n",
        "    self.Whc= default_rng(42).random((number_of_hiddens,number_of_hiddens))\n",
        "    self.bi=np.zeros(number_of_hiddens)\n",
        "    self.bf=np.zeros(number_of_hiddens)\n",
        "    self.bo=np.zeros(number_of_hiddens)\n",
        "    self.bc=np.zeros(number_of_hiddens)\n",
        "\n",
        "  def input_gate(self,X,Ht_1)  :\n",
        "    return logistic_sigmoid(np.matmul(X,self.Wxi)+np.matmul(Ht_1,self.Whi)+self.bi)\n",
        "\n",
        "  def output_gate(self,X,Ht_1) :\n",
        "    return logistic_sigmoid(np.matmul(X,self.Wxo)+np.matmul(Ht_1,self.Who)+self.bo)\n",
        "\n",
        "  def forget_gate(self,X,Ht_1) :\n",
        "    return logistic_sigmoid(np.matmul(X,self.Wxf)+np.matmul(Ht_1,self.Whf)+self.bf)\n",
        "\n",
        "  def Candidate_memory_Cell(self,X,Ht_1) :\n",
        "    return np.tanh(np.matmul(X,self.Wxc)+np.matmul(Ht_1,self.Whc)+self.bc)\n",
        "\n",
        "  def Memory_Cell(self,Ct_1) :\n",
        "    return self.forget_gate*Ct_1+self.input_gate*self.Candidate_memory_Cell\n",
        "\n",
        "  def Hidden_state(self) :\n",
        "    return self.output_gate*np.tanh(self.Memory_Cell)\n",
        "\n",
        "  def forward(self,X,Ht_1) :\n",
        "    I=self.input_gate(X,Ht_1)\n",
        "    F=self.forget_gate(X,Ht_1)\n",
        "    O=self.output_gate(X,Ht_1)\n",
        "    C_hat=self.Candidate_memory_Cell(X,Ht_1)\n",
        "    H=self.Hidden_state()\n",
        "    return H\n",
        "\n"
      ],
      "metadata": {
        "id": "HpDwPsbXBmEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's move forward,  and implement the other layer's or neural network like cnn, simple transformer, encoder-decoder ... to know how they work,\n",
        "## without forget the princple component the optimizations algorithms ."
      ],
      "metadata": {
        "id": "gRvN8-FkVwnG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}